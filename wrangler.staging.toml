# ====================================================================================
# BooksTrack Backend - Cloudflare Workers Configuration
# ====================================================================================
#
# Remote Bindings (Wrangler v4.37+)
# ---------------------------------------------------------------------------
# You can now access production resources during local development by setting
# `remote = true` on individual bindings (KV, R2, D1, etc.).
#
# Benefits:
#   - Test local code changes against real production data
#   - Share resources across development team
#   - Reproduce bugs tied to real data
#   - No need to deploy for every test iteration
#
# Usage:
#   1. Uncomment `remote = true` on any binding below
#   2. Run `npx wrangler dev` as normal
#   3. Your local code will connect to production resources
#
# Example:
#   [[kv_namespaces]]
#   binding = "CACHE"
#   id = "b9cade63b6db48fd80c109a013f38fdb"
#   remote = true  # âœ… Access production KV cache from local dev
#
# Documentation: .claude/WRANGLER_COMMAND_STANDARDS.md
# ====================================================================================

name = "api-worker-staging"
main = "src/index.js"
compatibility_date = "2024-10-01"
workers_dev = true
compatibility_flags = ["nodejs_compat"]

# Custom domain routes (oooefam.net)
routes = [
  { pattern = "staging-api.oooefam.net/*", zone_name = "oooefam.net" }
]

# Environment variables merged from all workers
[vars]
# Cache configuration (from books-api-proxy)
CACHE_HOT_TTL = "7200"         # 2 hours
CACHE_COLD_TTL = "1209600"     # 14 days
MAX_RESULTS_DEFAULT = "40"
RATE_LIMIT_MS = "50"
CONCURRENCY_LIMIT = "10"
AGGRESSIVE_CACHING = "true"

# Logging configuration (merged from all workers)
LOG_LEVEL = "DEBUG"
ENABLE_PERFORMANCE_LOGGING = "true"
ENABLE_CACHE_ANALYTICS = "true"
ENABLE_PROVIDER_METRICS = "true"
ENABLE_RATE_LIMIT_TRACKING = "true"
STRUCTURED_LOGGING = "true"

# External API configuration
OPENLIBRARY_BASE_URL = "https://openlibrary.org"
USER_AGENT = "BooksTracker/1.0 (nerd@ooheynerds.com) ExternalAPIsWorker/1.0.0"

# AI configuration (from bookshelf-ai-worker)
AI_PROVIDER = "gemini"  # or "cloudflare"
MAX_IMAGE_SIZE_MB = "10"
REQUEST_TIMEOUT_MS = "50000"
CONFIDENCE_THRESHOLD = "0.7"
MAX_SCAN_FILE_SIZE = "10485760"

# Response Envelope Format
# All API responses use the unified envelope format: { data, metadata, error? }
# Legacy format with success discriminator has been deprecated.
ENABLE_UNIFIED_ENVELOPE = "true"

# KV Namespaces (consolidated from books-api-proxy and external-apis-worker)
# Note: Set remote = true to access production KV during local development (Wrangler v4.37+)
[[kv_namespaces]]
binding = "CACHE"
id = "14e5ef1910904f0dbf16a31ea6d1be7c"

[[kv_namespaces]]
binding = "KV_CACHE"
id = "e9765c1aa8c24db2bbade660cf70ccd7"

# Secrets Store (for API keys from external-apis-worker and bookshelf-ai-worker)
[[secrets_store_secrets]]
binding = "GOOGLE_BOOKS_API_KEY"
store_id = "STAGING_SECRETS_STORE_ID" # Replace with actual staging secrets store ID
secret_name = "Google_books_hardoooe"

[[secrets_store_secrets]]
binding = "ISBNDB_API_KEY"
store_id = "STAGING_SECRETS_STORE_ID" # Replace with actual staging secrets store ID
secret_name = "ISBNDB_API_KEY"

[[secrets_store_secrets]]
binding = "GEMINI_API_KEY"
store_id = "STAGING_SECRETS_STORE_ID" # Replace with actual staging secrets store ID
secret_name = "google_gemini_oooebooks"

# R2 Buckets (from books-api-proxy and bookshelf-ai-worker)
[[r2_buckets]]
binding = "API_CACHE_COLD"
bucket_name = "personal-library-data-staging"

[[r2_buckets]]
binding = "LIBRARY_DATA"
bucket_name = "personal-library-data-staging"

[[r2_buckets]]
binding = "BOOKSHELF_IMAGES"
bucket_name = "bookshelf-images-staging"

[[r2_buckets]]
binding = "BOOK_COVERS"
bucket_name = "bookstrack-covers-staging"

# Workers AI binding (from books-api-proxy and bookshelf-ai-worker)
[ai]
binding = "AI"

# Durable Objects - SINGLE binding, NO service bindings!
[[durable_objects.bindings]]
name = "PROGRESS_WEBSOCKET_DO"
class_name = "ProgressWebSocketDO"

[[durable_objects.bindings]]
name = "RATE_LIMITER_DO"
class_name = "RateLimiterDO"

# Durable Object migrations
[[migrations]]
tag = "v1"
new_classes = ["ProgressWebSocketDO"]

[[migrations]]
tag = "v2"
new_classes = ["RateLimiterDO"]

# Analytics Engine (merged from books-api-proxy and bookshelf-ai-worker)
[[analytics_engine_datasets]]
binding = "PERFORMANCE_ANALYTICS"
dataset = "books_api_performance_staging"

[[analytics_engine_datasets]]
binding = "CACHE_ANALYTICS"
dataset = "books_api_cache_metrics_staging"

[[analytics_engine_datasets]]
binding = "PROVIDER_ANALYTICS"
dataset = "books_api_provider_performance_staging"

[[analytics_engine_datasets]]
binding = "AI_ANALYTICS"
dataset = "bookshelf_ai_performance_staging"

# Observability (from books-api-proxy and bookshelf-ai-worker)
[observability]
enabled = true
head_sampling_rate = 1.0

# Resource limits (from books-api-proxy and bookshelf-ai-worker)
[limits]
cpu_ms = 180000  # 3 minutes - increased from 30s to handle large enrichment batches
memory_mb = 256

# Placement (from books-api-proxy and bookshelf-ai-worker)
[placement]
mode = "smart"

# Queues for cache warming (Phase 2)
[[queues.producers]]
binding = "AUTHOR_WARMING_QUEUE"
queue = "author-warming-queue-staging"

[[queues.consumers]]
queue = "author-warming-queue-staging"
max_batch_size = 10
max_batch_timeout = 30
max_retries = 3
dead_letter_queue = "author-warming-dlq-staging"
max_concurrency = 5  # Process 5 batches in parallel

# Scheduled tasks (Phase 3 - R2 Cold Storage, Phase 4 - Alert Monitoring, ISBNdb Harvest)
[triggers]
crons = [
  "0 2 * * *",      # Daily archival at 2:00 AM UTC
  "*/15 * * * *",   # Alert checks every 15 minutes
  "0 3 * * *"       # Daily ISBNdb cover harvest at 3:00 AM UTC
]
