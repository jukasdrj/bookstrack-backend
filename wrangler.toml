# ====================================================================================
# BooksTrack Backend - Cloudflare Workers Configuration
# ====================================================================================
#
# Remote Bindings (Wrangler v4.37+)
# ---------------------------------------------------------------------------
# You can now access production resources during local development by setting
# `remote = true` on individual bindings (KV, R2, D1, etc.).
#
# Benefits:
#   - Test local code changes against real production data
#   - Share resources across development team
#   - Reproduce bugs tied to real data
#   - No need to deploy for every test iteration
#
# Usage:
#   1. Uncomment `remote = true` on any binding below
#   2. Run `npx wrangler dev` as normal
#   3. Your local code will connect to production resources
#
# Example:
#   [[kv_namespaces]]
#   binding = "CACHE"
#   id = "b9cade63b6db48fd80c109a013f38fdb"
#   remote = true  # âœ… Access production KV cache from local dev
#
# Documentation: .claude/WRANGLER_COMMAND_STANDARDS.md
# ====================================================================================

name = "api-worker"
main = "src/index.js"
compatibility_date = "2024-10-01"
workers_dev = true
compatibility_flags = ["nodejs_compat"]

# Custom domain routes (oooefam.net)
routes = [
  { pattern = "api.oooefam.net/*", zone_name = "oooefam.net" },
  { pattern = "harvest.oooefam.net/*", zone_name = "oooefam.net" }
]

# Environment variables merged from all workers
[vars]
# Cache configuration (from books-api-proxy)
CACHE_HOT_TTL = "7200"         # 2 hours
CACHE_COLD_TTL = "1209600"     # 14 days
MAX_RESULTS_DEFAULT = "40"
RATE_LIMIT_MS = "50"
CONCURRENCY_LIMIT = "10"
AGGRESSIVE_CACHING = "true"

# Logging configuration (merged from all workers)
LOG_LEVEL = "DEBUG"
ENABLE_PERFORMANCE_LOGGING = "true"
ENABLE_CACHE_ANALYTICS = "true"
ENABLE_PROVIDER_METRICS = "true"
ENABLE_RATE_LIMIT_TRACKING = "true"
STRUCTURED_LOGGING = "true"

# External API configuration
OPENLIBRARY_BASE_URL = "https://openlibrary.org"
USER_AGENT = "BooksTracker/1.0 (nerd@ooheynerds.com) ExternalAPIsWorker/1.0.0"

# AI configuration (from bookshelf-ai-worker)
AI_PROVIDER = "gemini"  # or "cloudflare"
MAX_IMAGE_SIZE_MB = "10"
REQUEST_TIMEOUT_MS = "50000"
CONFIDENCE_THRESHOLD = "0.7"
MAX_SCAN_FILE_SIZE = "10485760"

# Response Envelope Format
# All API responses use the unified envelope format: { data, metadata, error? }
# Legacy format with success discriminator has been deprecated.
ENABLE_UNIFIED_ENVELOPE = "true"

# Durable Object Architecture Refactoring (Phase 2 - Architectural Cleanup)
# When true: Uses refactored architecture (WebSocketConnectionDO + JobStateManagerDO + Services)
# When false: Uses legacy monolithic ProgressWebSocketDO (default for backward compatibility)
ENABLE_REFACTORED_DOS = "false"  # Default: legacy architecture

# KV Namespaces (consolidated from books-api-proxy and external-apis-worker)
# Note: Set remote = true to access production KV during local development (Wrangler v4.37+)
[[kv_namespaces]]
binding = "CACHE"
id = "b9cade63b6db48fd80c109a013f38fdb"
# remote = false  # Default: use local simulation (empty cache)
# remote = true   # Uncomment to access production KV cache from 'npx wrangler dev'

[[kv_namespaces]]
binding = "KV_CACHE"
id = "b9cade63b6db48fd80c109a013f38fdb"
# remote = false  # Default: use local simulation
# remote = true   # Uncomment to access production KV cache

# Note: SCAN_JOBS KV namespace (5d4b89403bbb4be1949b1ee30df5353e) is intentionally
# excluded - we're eliminating the polling system in favor of WebSocket-only

# Secrets Store (for API keys from external-apis-worker and bookshelf-ai-worker)
[[secrets_store_secrets]]
binding = "GOOGLE_BOOKS_API_KEY"
store_id = "b0562ac16fde468c8af12717a6c88400"
secret_name = "Google_books_hardoooe"

[[secrets_store_secrets]]
binding = "ISBNDB_API_KEY"
store_id = "b0562ac16fde468c8af12717a6c88400"
secret_name = "ISBNDB_API_KEY"

[[secrets_store_secrets]]
binding = "GEMINI_API_KEY"
store_id = "b0562ac16fde468c8af12717a6c88400"
secret_name = "google_gemini_oooebooks"

# Note: CF_ACCOUNT_ID and CF_API_TOKEN are stored as Worker secrets (via `wrangler secret put`)
# These are automatically available as env.CF_ACCOUNT_ID and env.CF_API_TOKEN

# R2 Buckets (from books-api-proxy and bookshelf-ai-worker)
# Note: Set remote = true to access production R2 during local development (Wrangler v4.37+)
[[r2_buckets]]
binding = "API_CACHE_COLD"
bucket_name = "personal-library-data"
# remote = false  # Default: local simulation (empty bucket)
# remote = true   # Uncomment to access production R2 from 'npx wrangler dev'

[[r2_buckets]]
binding = "LIBRARY_DATA"
bucket_name = "personal-library-data"
# remote = false  # Default: local simulation
# remote = true   # Uncomment to access production R2

[[r2_buckets]]
binding = "BOOKSHELF_IMAGES"
bucket_name = "bookshelf-images"
# remote = false  # Default: local simulation
# remote = true   # Uncomment to access production R2

[[r2_buckets]]
binding = "BOOK_COVERS"
bucket_name = "bookstrack-covers"
# remote = false  # Default: local simulation
# remote = true   # Uncomment to access production R2

# Workers AI binding (from books-api-proxy and bookshelf-ai-worker)
[ai]
binding = "AI"

# Rate Limiting via Durable Objects (Security: Prevents denial-of-wallet attacks)
# Implementation: src/durable-objects/rate-limiter.js provides atomic per-IP counters
# Binding: RATE_LIMITER_DO (one DO instance per client IP for serialization)
# Limit: 10 requests per 60-second window per IP (protects expensive AI/enrichment endpoints)
# Cost: ~$0 (DO requests included in Workers plan with ~100 calls/min peak)

# CORS Configuration
# Current: 'Access-Control-Allow-Origin: *' (permissive)
# Reason: Primary client is native iOS app (doesn't send Origin header)
# Defense: Rate limiting (10 req/min) prevents abuse
# Phase 2: Restrict CORS when web interface is added (see src/middleware/cors.js)

# Durable Objects - SINGLE binding, NO service bindings!
[[durable_objects.bindings]]
name = "PROGRESS_WEBSOCKET_DO"
class_name = "ProgressWebSocketDO"

[[durable_objects.bindings]]
name = "RATE_LIMITER_DO"
class_name = "RateLimiterDO"

# New refactored Durable Objects (Phase 2: Architectural Refactoring)
[[durable_objects.bindings]]
name = "WEBSOCKET_CONNECTION_DO"
class_name = "WebSocketConnectionDO"

[[durable_objects.bindings]]
name = "JOB_STATE_MANAGER_DO"
class_name = "JobStateManagerDO"

# Durable Object migrations
[[migrations]]
tag = "v1"
new_classes = ["ProgressWebSocketDO"]

[[migrations]]
tag = "v2"
new_classes = ["RateLimiterDO"]

[[migrations]]
tag = "v3"
new_classes = ["WebSocketConnectionDO", "JobStateManagerDO"]

# Analytics Engine (merged from books-api-proxy and bookshelf-ai-worker)
[[analytics_engine_datasets]]
binding = "PERFORMANCE_ANALYTICS"
dataset = "books_api_performance"

[[analytics_engine_datasets]]
binding = "CACHE_ANALYTICS"
dataset = "books_api_cache_metrics"

[[analytics_engine_datasets]]
binding = "PROVIDER_ANALYTICS"
dataset = "books_api_provider_performance"

[[analytics_engine_datasets]]
binding = "AI_ANALYTICS"
dataset = "bookshelf_ai_performance"

# Observability (from books-api-proxy and bookshelf-ai-worker)
[observability]
enabled = true
head_sampling_rate = 1.0

# Resource limits (from books-api-proxy and bookshelf-ai-worker)
[limits]
cpu_ms = 180000  # 3 minutes - increased from 30s to handle large enrichment batches
memory_mb = 256

# Placement (from books-api-proxy and bookshelf-ai-worker)
[placement]
mode = "smart"

# Queues for cache warming (Phase 2)
[[queues.producers]]
binding = "AUTHOR_WARMING_QUEUE"
queue = "author-warming-queue"

[[queues.consumers]]
queue = "author-warming-queue"
max_batch_size = 10
max_batch_timeout = 30
max_retries = 3
dead_letter_queue = "author-warming-dlq"
max_concurrency = 5  # Process 5 batches in parallel

# Scheduled tasks (Phase 3 - R2 Cold Storage, Phase 4 - Alert Monitoring, ISBNdb Harvest)
[triggers]
crons = [
  "0 2 * * *",      # Daily archival at 2:00 AM UTC
  "*/15 * * * *",   # Alert checks every 15 minutes
  "0 3 * * *"       # Daily ISBNdb cover harvest at 3:00 AM UTC
]

# ====================================================================================
# STAGING ENVIRONMENT (Phase 3 - Response Envelope Unification Testing)
# ====================================================================================

[env.staging]
name = "api-worker-staging"
routes = [
  { pattern = "staging-api.oooefam.net/*", zone_name = "oooefam.net" }
]

[[env.staging.kv_namespaces]]
binding = "CACHE"
id = "STAGING_KV_ID_1"

[[env.staging.kv_namespaces]]
binding = "KV_CACHE"
id = "STAGING_KV_ID_1"

[[env.staging.r2_buckets]]
binding = "API_CACHE_COLD"
bucket_name = "personal-library-data-staging"

[[env.staging.r2_buckets]]
binding = "LIBRARY_DATA"
bucket_name = "personal-library-data-staging"

[[env.staging.r2_buckets]]
binding = "BOOKSHELF_IMAGES"
bucket_name = "bookshelf-images-staging"

[[env.staging.r2_buckets]]
binding = "BOOK_COVERS"
bucket_name = "bookstrack-covers-staging"

[[env.staging.durable_objects.bindings]]
name = "PROGRESS_WEBSOCKET_DO"
class_name = "ProgressWebSocketDO"

[[env.staging.durable_objects.bindings]]
name = "RATE_LIMITER_DO"
class_name = "RateLimiterDO"

[[env.staging.analytics_engine_datasets]]
binding = "PERFORMANCE_ANALYTICS"
dataset = "books_api_performance_staging"

[[env.staging.analytics_engine_datasets]]
binding = "CACHE_ANALYTICS"
dataset = "books_api_cache_metrics_staging"

[[env.staging.analytics_engine_datasets]]
binding = "PROVIDER_ANALYTICS"
dataset = "books_api_provider_performance_staging"

[[env.staging.analytics_engine_datasets]]
binding = "AI_ANALYTICS"
dataset = "bookshelf_ai_performance_staging"

[[env.staging.queues.producers]]
binding = "AUTHOR_WARMING_QUEUE"
queue = "author-warming-queue-staging"

[[env.staging.queues.consumers]]
queue = "author-warming-queue-staging"
dead_letter_queue = "author-warming-dlq-staging"
